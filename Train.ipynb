{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Embedding, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword  ...                                               text target\n",
       "0    1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
       "1    4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
       "2    5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
       "3    6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
       "4    7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
       "5    8     NaN  ...  #RockyFire Update => California Hwy. 20 closed...      1\n",
       "6   10     NaN  ...  #flood #disaster Heavy rain causes flash flood...      1\n",
       "7   13     NaN  ...  I'm on top of the hill and I can see a fire in...      1\n",
       "8   14     NaN  ...  There's an emergency evacuation happening now ...      1\n",
       "9   15     NaN  ...  I'm afraid that the tornado is coming to our a...      1\n",
       "10  16     NaN  ...        Three people died from the heat wave so far      1\n",
       "11  17     NaN  ...  Haha South Tampa is getting flooded hah- WAIT ...      1\n",
       "12  18     NaN  ...  #raining #flooding #Florida #TampaBay #Tampa 1...      1\n",
       "13  19     NaN  ...            #Flood in Bago Myanmar #We arrived Bago      1\n",
       "14  20     NaN  ...  Damage to school bus on 80 in multi car crash ...      1\n",
       "\n",
       "[15 rows x 5 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/train.csv')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7552</td>\n",
       "      <td>5080</td>\n",
       "      <td>7613</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>221</td>\n",
       "      <td>3341</td>\n",
       "      <td>7503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>USA</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  ...      target\n",
       "count    7613.000000  ...  7613.00000\n",
       "unique           NaN  ...         NaN\n",
       "top              NaN  ...         NaN\n",
       "freq             NaN  ...         NaN\n",
       "mean     5441.934848  ...     0.42966\n",
       "std      3137.116090  ...     0.49506\n",
       "min         1.000000  ...     0.00000\n",
       "25%      2734.000000  ...     0.00000\n",
       "50%      5408.000000  ...     0.00000\n",
       "75%      8146.000000  ...     1.00000\n",
       "max     10873.000000  ...     1.00000\n",
       "\n",
       "[11 rows x 5 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 'id' is not going to help us with the prediction, we can drop it straight away.\n",
    "Furthermore, we will look straight away into getting the prediction purely from the tweet content as we have a lot of missing value, so we'll drop the keyword & location column.\n",
    "\n",
    "It seems like we have some duplicate within the text column too, we'll look into that. Do they have the same target? If they do we'll remove them to have just one of each text, if they don't we need to look into why they don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "4290  #Allah describes piling up #wealth thinking it...       0\n",
       "4299  #Allah describes piling up #wealth thinking it...       0\n",
       "4312  #Allah describes piling up #wealth thinking it...       1\n",
       "6363  #Bestnaijamade: 16yr old PKK suicide bomber wh...       1\n",
       "6373  #Bestnaijamade: 16yr old PKK suicide bomber wh...       1\n",
       "...                                                 ...     ...\n",
       "6091  that horrible sinking feeling when youÛªve be...       1\n",
       "6103  that horrible sinking feeling when youÛªve be...       0\n",
       "6094  that horrible sinking feeling when youÛªve be...       0\n",
       "5641  wowo--=== 12000 Nigerian refugees repatriated ...       0\n",
       "5620  wowo--=== 12000 Nigerian refugees repatriated ...       1\n",
       "\n",
       "[179 rows x 2 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping aforementionned columns\n",
    "df.drop(['id','keyword','location'],axis=1,inplace=True)\n",
    "\n",
    "# Keep = False is to avoid that it keeps the first instance of each duplicate which would result in duplicates remaining\n",
    "#Looking at duplicated text\n",
    "df[df['text'].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some have the same target, some don't. If they have the same target, we'll clean up such that we have just one of each, if they don't we'll remove them altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[4290, 4299, 4312]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[4221, 4239, 4244]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[2830, 2831, 2832, 2833]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[4597, 4605, 4618]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[4232, 4235]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[3240, 3243, 3248, 3251, 3261, 3266]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hellfire is surrounded by desires so be carefu...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[4285, 4305, 4313]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hellfire! We donÛªt even want to think about ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[4306, 4320]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I Pledge Allegiance To The P.O.P.E. And The Bu...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[1214, 1365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In #islam saving a person is equal in reward t...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[6614, 6616]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mmmmmm I'm burning.... I'm burning buildings I...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[1197, 1331]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RT NotExplained: The only known image of infam...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[4379, 4381]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Prophet (peace be upon him) said 'Save you...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[4284, 4286, 4292, 4304, 4309, 4318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[610, 624, 630, 634]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Who is bringing the tornadoes and floods. Who ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[3985, 4013, 4019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>like for the music video I want some real acti...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[1221, 1349]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[6091, 6094, 6103, 6123]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[5620, 5641]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  ...                                 index\n",
       "0   #Allah describes piling up #wealth thinking it...  ...                    [4290, 4299, 4312]\n",
       "1   #foodscare #offers2go #NestleIndia slips into ...  ...                    [4221, 4239, 4244]\n",
       "2   .POTUS #StrategicPatience is a strategy for #G...  ...              [2830, 2831, 2832, 2833]\n",
       "3   CLEARED:incident with injury:I-495  inner loop...  ...                    [4597, 4605, 4618]\n",
       "4   Caution: breathing may be hazardous to your he...  ...                          [4232, 4235]\n",
       "5   He came to a land which was engulfed in tribal...  ...  [3240, 3243, 3248, 3251, 3261, 3266]\n",
       "6   Hellfire is surrounded by desires so be carefu...  ...                    [4285, 4305, 4313]\n",
       "7   Hellfire! We donÛªt even want to think about ...  ...                          [4306, 4320]\n",
       "8   I Pledge Allegiance To The P.O.P.E. And The Bu...  ...                          [1214, 1365]\n",
       "9   In #islam saving a person is equal in reward t...  ...                          [6614, 6616]\n",
       "10  Mmmmmm I'm burning.... I'm burning buildings I...  ...                          [1197, 1331]\n",
       "11  RT NotExplained: The only known image of infam...  ...                          [4379, 4381]\n",
       "12  The Prophet (peace be upon him) said 'Save you...  ...  [4284, 4286, 4292, 4304, 4309, 4318]\n",
       "13                         To fight bioterrorism sir.  ...                  [610, 624, 630, 634]\n",
       "14  Who is bringing the tornadoes and floods. Who ...  ...                    [3985, 4013, 4019]\n",
       "15  like for the music video I want some real acti...  ...                          [1221, 1349]\n",
       "16  that horrible sinking feeling when youÛªve be...  ...              [6091, 6094, 6103, 6123]\n",
       "17  wowo--=== 12000 Nigerian refugees repatriated ...  ...                          [5620, 5641]\n",
       "\n",
       "[18 rows x 3 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['index'] = df.index # Will help to make list of originals index to pick up the data after\n",
    "\n",
    "aggs = {'target': 'mean',\n",
    "         'index':lambda x: list(x)} # Keep track of the index \n",
    "\n",
    "grouped_df = df[df['text'].duplicated(keep=False)].groupby('text').agg(aggs) \n",
    "\n",
    "# if target mean is not equal to 0 or 1 then the target is not consistent throughout the observations, let's see which one they are\n",
    "dif_targets = grouped_df[(grouped_df.target!=1) & (grouped_df.target!=0)].reset_index()\n",
    "dif_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take fix the target manually but given there is only 18 of them and some are ambiguous, we will proceed with removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We dropped 55 observations\n"
     ]
    }
   ],
   "source": [
    "# We'll save down the column of index as a list for us to drop them\n",
    "to_drop_dif_targets = dif_targets['index'].tolist()\n",
    "to_drop_dif_targets = [item for sublist in to_drop_dif_targets for item in sublist] #flatten nested list\n",
    "\n",
    "# Dropping duplicates with  target\n",
    "shape = df.shape[0]\n",
    "df.drop(to_drop_dif_targets,inplace=True)\n",
    "dropped =  shape - df.shape[0]\n",
    "\n",
    "print(f\"We dropped {dropped} observations\")\n",
    "\n",
    "\n",
    "# Cleaning up\n",
    "df.drop('index',axis=1,inplace=True) # no longer need that column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[6363, 6366, 6373, 6377, 6378, 6392]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#KCA #VoteJKT48ID 12News: UPDATE: A family of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2822, 2828]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Myanmar  Displaced #Rohingya at #Sittwe point...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2816, 2841]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Newswatch: 2 vehicles collided at Lock and La...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1704, 1725]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#SigAlert: North &amp;amp; Southbound 133 closed b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3790, 3795]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ...                                 index\n",
       "0  #Bestnaijamade: 16yr old PKK suicide bomber wh...  ...  [6363, 6366, 6373, 6377, 6378, 6392]\n",
       "1  #KCA #VoteJKT48ID 12News: UPDATE: A family of ...  ...                          [2822, 2828]\n",
       "2  #Myanmar  Displaced #Rohingya at #Sittwe point...  ...                          [2816, 2841]\n",
       "3  #Newswatch: 2 vehicles collided at Lock and La...  ...                          [1704, 1725]\n",
       "4  #SigAlert: North &amp; Southbound 133 closed b...  ...                          [3790, 3795]\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now look at the one that have the same target\n",
    "# For those we'll keep only one of each to avoid any bias\n",
    "same_targets = grouped_df[(grouped_df.target==1) | (grouped_df.target==0)].reset_index()\n",
    "\n",
    "print(same_targets.shape)\n",
    "same_targets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We dropped 73 observations\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicates above keeping only one instance of each tweet\n",
    "\n",
    "shape = df.shape[0]\n",
    "\n",
    "df.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "dropped =  shape - df.shape[0]\n",
    "print(\"We dropped {} observations\".format(dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a final look at the dataset before we proceed to verify everything seems cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7485</td>\n",
       "      <td>7485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  target\n",
       "count   7485  7485.0\n",
       "unique  7485     NaN"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all').iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.574081\n",
       "1    0.425919\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True) # Checking for balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Forest fire near La Ronge Sask. Canada\n",
      "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "13,000 people receive #wildfires evacuation orders in California \n",
      "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
      "#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\n",
      "I'm on top of the hill and I can see a fire in the woods...\n",
      "There's an emergency evacuation happening now in the building across the street\n",
      "I'm afraid that the tornado is coming to our area...\n",
      "Three people died from the heat wave so far\n",
      "Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding\n",
      "#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count \n",
      "#Flood in Bago Myanmar #We arrived Bago\n",
      "Damage to school bus on 80 in multi car crash #BREAKING \n",
      "What's up man?\n",
      "I love fruits\n",
      "Summer is lovely\n",
      "My car is so fast\n",
      "What a goooooooaaaaaal!!!!!!\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True,drop=True) # deals with some missing index\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    print(df['text'][i]) # allows to see the complete text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks better, no more duplicated values. There is no missing values so we won't need to remove any more observations. \n",
    "\n",
    "The dataset is fairly balanced - about 42% of the tweets indicate a disaster.\n",
    "\n",
    "Let's clean up the texts a bit. There's a lot of links, but those will get out by themselves once we limit the vocabulary size as they are all unique.  \n",
    "\n",
    "Furthermore, we'll remove ponctuation, clean up if there's any double space, put everything in lowercase and remove the stop words, along with lemmatising the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>rockyfire update   california hwy 20 close dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain cause flash flooding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "      <td>m hill fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "      <td>s emergency evacuation happen building street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "      <td>m afraid tornado come area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "      <td>people die heat wave far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "      <td>haha south tampa getting flood hah wait second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>rain flood florida tampabay tampa 18 19 day ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "      <td>flood bago myanmar arrive bago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "      <td>damage school bus 80 multi car crash breaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "      <td>s man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "      <td>love fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>summer lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "      <td>car fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>goooooooaaaaaal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  ...                                         text_clean\n",
       "0   Our Deeds are the Reason of this #earthquake M...  ...               deed reason earthquake allah forgive\n",
       "1              Forest fire near La Ronge Sask. Canada  ...              forest fire near la ronge sask canada\n",
       "2   All residents asked to 'shelter in place' are ...  ...  resident ask shelter place notify officer evac...\n",
       "3   13,000 people receive #wildfires evacuation or...  ...  13000 people receive wildfire evacuation order...\n",
       "4   Just got sent this photo from Ruby #Alaska as ...  ...  got send photo ruby alaska smoke wildfires pou...\n",
       "5   #RockyFire Update => California Hwy. 20 closed...  ...  rockyfire update   california hwy 20 close dir...\n",
       "6   #flood #disaster Heavy rain causes flash flood...  ...  flood disaster heavy rain cause flash flooding...\n",
       "7   I'm on top of the hill and I can see a fire in...  ...                                   m hill fire wood\n",
       "8   There's an emergency evacuation happening now ...  ...      s emergency evacuation happen building street\n",
       "9   I'm afraid that the tornado is coming to our a...  ...                         m afraid tornado come area\n",
       "10        Three people died from the heat wave so far  ...                           people die heat wave far\n",
       "11  Haha South Tampa is getting flooded hah- WAIT ...  ...  haha south tampa getting flood hah wait second...\n",
       "12  #raining #flooding #Florida #TampaBay #Tampa 1...  ...  rain flood florida tampabay tampa 18 19 day ve...\n",
       "13            #Flood in Bago Myanmar #We arrived Bago  ...                     flood bago myanmar arrive bago\n",
       "14  Damage to school bus on 80 in multi car crash ...  ...      damage school bus 80 multi car crash breaking\n",
       "15                                     What's up man?  ...                                              s man\n",
       "16                                      I love fruits  ...                                         love fruit\n",
       "17                                   Summer is lovely  ...                                      summer lovely\n",
       "18                                  My car is so fast  ...                                           car fast\n",
       "19                       What a goooooooaaaaaal!!!!!!  ...                                    goooooooaaaaaal\n",
       "\n",
       "[20 rows x 3 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Loading english language elements from spacy\n",
    "\n",
    "df['text_clean'] = df[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "df['text_clean'] = df[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "df['text_clean'] = df[\"text_clean\"].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)]))\n",
    "\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data & tokenizing the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the dataset is already split into train / test, the test one is only to submit to Kaggle and it does not have a target, making it complicated to evalue our model. \n",
    "\n",
    "We should thus train/test the train dataset to choose the best model. We'll also keep 10% for final testing to have an idea of how we expect our model to perform. \n",
    "The proportion are thus as follow:\n",
    "\n",
    "    * Train: 75%\n",
    "\n",
    "    * Validation: 15%\n",
    "    \n",
    "    * Test: 10%\n",
    "\n",
    "We'll also both set a random state, to ensure we are always working with the same split, and stratify with respect to the target as we are working with a classification problem. \n",
    "\n",
    "However, before submitting to Kaggle, we'll retrain the model on the entire dataset as to not 'waste' any data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "(5613,) (5613,)\n",
      "Validation:\n",
      "(1124,) (1124,)\n",
      "Testing:\n",
      "(748,) (748,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df.text_clean, df.target ,test_size=0.25,random_state = 0, stratify = df.target)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test ,test_size=0.6,random_state = 0, stratify = Y_test) # 0.6 * 0.25 = 0.15\n",
    " \n",
    "print('Training:')\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print('Validation:')\n",
    "print(X_valid.shape, Y_valid.shape)\n",
    "print('Testing:')\n",
    "print(X_test.shape,Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000, oov_token=\"<OOV>\") # instanciate the tokenizer\n",
    "\n",
    "tokenizer.fit_on_texts(X_train) # fitting tokenizer only on train dataset to prevent any data leakage\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is  25\n"
     ]
    }
   ],
   "source": [
    "# Finding out the max length out of all the texts\n",
    "MAX_LENGTH = len(max(X_train + X_test + X_valid, key =  lambda i: len(i) ))\n",
    "print(\"The max length is \",MAX_LENGTH)\n",
    "\n",
    "# Padding the sequences \n",
    "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, padding=\"post\", maxlen= MAX_LENGTH)\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding=\"post\", maxlen= MAX_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\", maxlen= MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[777   1  14 ...   0   0   0]\n",
      " [247  17 134 ...   0   0   0]\n",
      " [778  92   5 ...   0   0   0]\n",
      " ...\n",
      " [  1   1 910 ...   0   0   0]\n",
      " [ 51 310   1 ...   0   0   0]\n",
      " [104   1 566 ...   0   0   0]], shape=(124, 25), dtype=int32) tf.Tensor(\n",
      "[1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
      " 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0], shape=(124,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 124\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train,Y_train)).batch(BATCH_SIZE)\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test,Y_test.values)).batch(BATCH_SIZE)\n",
    "valid = tf.data.Dataset.from_tensor_slices((X_test,Y_test.values)).batch(BATCH_SIZE)\n",
    "\n",
    "for text, target in train.take(1):\n",
    "  print(text, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is all pre-processed, we can move on to the model.\n",
    "\n",
    "# Recurrent Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                  # Word Embedding layer           \n",
    "                  Embedding(vocab_size, 64,input_shape=[MAX_LENGTH,]),\n",
    "                  # Gobal average pooling\n",
    "                  SimpleRNN(units=64, return_sequences=True), # maintains the sequential nature\n",
    "                  SimpleRNN(units=32, return_sequences=False), # returns the last output\n",
    "                  # Dense layers once the data is flat\n",
    "                  Dense(16, activation='relu'),\n",
    "                  Dense(8, activation='relu'),\n",
    "\n",
    "                  # output layer with as many neurons as the number of classes\n",
    "                  # for the target variable and softmax activation\n",
    "                  Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 25, 64)            1051008   \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 25, 64)            8256      \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 32)                3104      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,063,041\n",
      "Trainable params: 1,063,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss = tf.keras.losses.BinaryCrossentropy(), # bc binary case here\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "46/46 [==============================] - 12s 79ms/step - loss: 0.6170 - binary_accuracy: 0.6551 - val_loss: 0.5237 - val_binary_accuracy: 0.7754\n",
      "Epoch 2/25\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 0.4635 - binary_accuracy: 0.7990 - val_loss: 0.4868 - val_binary_accuracy: 0.7660\n",
      "Epoch 3/25\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 0.3912 - binary_accuracy: 0.8409 - val_loss: 0.5282 - val_binary_accuracy: 0.7607\n",
      "Epoch 4/25\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.3271 - binary_accuracy: 0.8764 - val_loss: 0.5368 - val_binary_accuracy: 0.7848\n",
      "Epoch 5/25\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 0.3160 - binary_accuracy: 0.8799 - val_loss: 0.5824 - val_binary_accuracy: 0.7674\n",
      "Epoch 6/25\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.2944 - binary_accuracy: 0.8863 - val_loss: 0.5344 - val_binary_accuracy: 0.7968\n",
      "Epoch 7/25\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 0.2500 - binary_accuracy: 0.9097 - val_loss: 0.5949 - val_binary_accuracy: 0.7901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3b320dc40>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,\n",
    "          epochs=25, \n",
    "          validation_data=valid,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/model_RNN.h5\") # model_simpleRNN_1 was done on batch size = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = tf.keras.Sequential([\n",
    "                  Embedding(vocab_size+1, 64, input_shape=[MAX_LENGTH,],name=\"embedding\"),\n",
    "                  GRU(units=64, return_sequences=True), # maintains the sequential nature\n",
    "                  GRU(units=32, return_sequences=False), # returns the last output\n",
    "                  Dense(16, activation='relu'),\n",
    "                  Dense(8, activation='relu'),\n",
    "\n",
    "                  Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 64)            1051072   \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 25, 64)            24960     \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,086,113\n",
      "Trainable params: 1,086,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "model_gru.compile(optimizer=optimizer, \n",
    "              loss = tf.keras.losses.BinaryCrossentropy(), # bc binary case here\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "46/46 [==============================] - 13s 103ms/step - loss: 0.6833 - binary_accuracy: 0.5740 - val_loss: 0.6823 - val_binary_accuracy: 0.5749\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 4s 86ms/step - loss: 0.6798 - binary_accuracy: 0.5740 - val_loss: 0.6658 - val_binary_accuracy: 0.5789\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 4s 85ms/step - loss: 0.5318 - binary_accuracy: 0.7408 - val_loss: 0.4566 - val_binary_accuracy: 0.7968\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 4s 88ms/step - loss: 0.4302 - binary_accuracy: 0.8142 - val_loss: 0.4492 - val_binary_accuracy: 0.7861\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 4s 88ms/step - loss: 0.3928 - binary_accuracy: 0.8382 - val_loss: 0.4790 - val_binary_accuracy: 0.7981\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 4s 78ms/step - loss: 0.3766 - binary_accuracy: 0.8459 - val_loss: 0.5218 - val_binary_accuracy: 0.7941\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3778 - binary_accuracy: 0.8493 - val_loss: 0.5088 - val_binary_accuracy: 0.7914\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.3703 - binary_accuracy: 0.8559 - val_loss: 0.4882 - val_binary_accuracy: 0.7914\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 4s 83ms/step - loss: 0.3578 - binary_accuracy: 0.8653 - val_loss: 0.5124 - val_binary_accuracy: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe251d89730>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.fit(train,\n",
    "              epochs=20, \n",
    "              validation_data=valid,\n",
    "              callbacks=callbacks\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.save('models/model_GRU.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = tf.keras.Sequential([\n",
    "                  Embedding(vocab_size+1, 64, input_shape=[MAX_LENGTH],name=\"embedding\"),\n",
    "                  LSTM(units=64, return_sequences=True), # maintains the sequential nature\n",
    "                  LSTM(units=32, return_sequences=False), # returns the last output\n",
    "                  Dense(16, activation='relu'),\n",
    "                  Dense(8, activation='relu'),\n",
    "                  Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 64)            1051072   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 25, 64)            33024     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,097,185\n",
      "Trainable params: 1,097,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "model_lstm.compile(optimizer=optimizer, \n",
    "              loss = tf.keras.losses.BinaryCrossentropy(), # bc binary case here\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 10, \n",
    "                                        restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "46/46 [==============================] - 12s 125ms/step - loss: 0.6594 - binary_accuracy: 0.5979 - val_loss: 0.5598 - val_binary_accuracy: 0.7741\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 4s 88ms/step - loss: 0.4754 - binary_accuracy: 0.7924 - val_loss: 0.4471 - val_binary_accuracy: 0.7941\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 3s 75ms/step - loss: 0.4079 - binary_accuracy: 0.8267 - val_loss: 0.4587 - val_binary_accuracy: 0.7968\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 3s 70ms/step - loss: 0.3804 - binary_accuracy: 0.8382 - val_loss: 0.4876 - val_binary_accuracy: 0.7941\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 4s 78ms/step - loss: 0.3692 - binary_accuracy: 0.8468 - val_loss: 0.5460 - val_binary_accuracy: 0.7901\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 4s 81ms/step - loss: 0.3697 - binary_accuracy: 0.8523 - val_loss: 0.5506 - val_binary_accuracy: 0.7807\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 4s 96ms/step - loss: 0.3748 - binary_accuracy: 0.8509 - val_loss: 0.5047 - val_binary_accuracy: 0.7781\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 4s 87ms/step - loss: 0.3568 - binary_accuracy: 0.8637 - val_loss: 0.5282 - val_binary_accuracy: 0.7687\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 4s 97ms/step - loss: 0.3456 - binary_accuracy: 0.8696 - val_loss: 0.5234 - val_binary_accuracy: 0.7807\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 4s 85ms/step - loss: 0.3456 - binary_accuracy: 0.8701 - val_loss: 0.5319 - val_binary_accuracy: 0.7767\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 4s 94ms/step - loss: 0.3508 - binary_accuracy: 0.8655 - val_loss: 0.5526 - val_binary_accuracy: 0.7807\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 4s 89ms/step - loss: 0.3485 - binary_accuracy: 0.8637 - val_loss: 0.5574 - val_binary_accuracy: 0.7754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe252e03130>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(train,\n",
    "                epochs = 30,\n",
    "                validation_data = valid,\n",
    "                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save('models/model_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model (Distilbert)\n",
    "\n",
    "Tried to finetune a model from transformers. I used distilbert rather than the original BERT as it has much faster training time. The model is said to 'reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster'[Sanh et al., 2020]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_519', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instanciate model and tokenizer\n",
    "distil_bert_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "distil_bert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)\n",
    "\n",
    "\n",
    "# Because Bert/Distilbert have their own tokenizer, we'll need to retokenizer the dataset from the start\n",
    "# However, keeping same proportion and same random state to ensure same data in each split\n",
    "\n",
    "\n",
    "    # Splitting dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(df.text_clean, df.target ,test_size=0.25,random_state = 0, stratify = df.target)\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test ,test_size=0.6,random_state = 0, stratify = y_test) # 0.6 * 0.25 = 0.15\n",
    "\n",
    "\n",
    "\n",
    "    # Both train_encodings & val are dict-like object with both the tokens and the attention masks \n",
    "train_encodings = distil_bert_tokenizer(x_train.to_list(),\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "val_encodings = distil_bert_tokenizer(x_valid.to_list(),\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "x_test = distil_bert_tokenizer_2(x_test.tolist(),\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\" # Will be able to use directly for preds after\n",
    "                                 ) \n",
    "\n",
    "    # Creating TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_valid\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 3820s 84s/step - loss: 0.4179 - accuracy: 0.8283 - val_loss: 0.4006 - val_accuracy: 0.8381\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 3314s 72s/step - loss: 0.3332 - accuracy: 0.8692 - val_loss: 0.4168 - val_accuracy: 0.8274\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 2239s 48s/step - loss: 0.2593 - accuracy: 0.9029 - val_loss: 0.4607 - val_accuracy: 0.8301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3b452ddf0>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 124\n",
    "\n",
    "# Compiling model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # Low learning rate for finetuning\n",
    "\n",
    "distil_bert_model.compile(optimizer=optimizer, \n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "# Creating an early stopper\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 2, # Lower patience as model converges much more quicklu & each epoch takes much longer\n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "# Training\n",
    "distil_bert_model.fit(train_dataset.shuffle(100).batch(BATCH_SIZE),\n",
    "          epochs=10,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          validation_data=val_dataset.shuffle(100).batch(BATCH_SIZE),\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "distil_bert_model.save_pretrained(\"./models/distil_bert_weights\") # Saved a different way as it is a subclassed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "Because the Kaggle competition is judged on F1 score, we'll use that metric to compare the models as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./models/distil_bert_weights were not used when initializing TFDistilBertForSequenceClassification: ['dropout_519']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./models/distil_bert_weights and are newly initialized: ['dropout_659']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN F1 score:\n",
      "0.7278382581648523\n",
      "GRU F1 score:\n",
      "0.7163120567375887\n",
      "LSTM F1 score:\n",
      "0.7344827586206896\n",
      "BERT F1 score:\n",
      "0.7826086956521738\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load the different models\n",
    "RNN_model = tf.keras.models.load_model('models/model_RNN.h5')\n",
    "GRU_model = tf.keras.models.load_model('models/model_GRU.h5')\n",
    "LSTM_model = tf.keras.models.load_model('models/model_lstm.h5')\n",
    "Dist_BERT_model = TFDistilBertForSequenceClassification.from_pretrained(\"./models/distil_bert_weights\")\n",
    "\n",
    "## Setting up \n",
    "actual_values = Y_test \n",
    "threshold = 0.5\n",
    "\n",
    "# Get the predictions\n",
    "pred_RNN  = RNN_model.predict(X_test)\n",
    "pred_GRU  = GRU_model.predict(X_test)\n",
    "pred_LSTM = LSTM_model.predict(X_test)\n",
    "pred_BERT = Dist_BERT_model.predict(x_test['input_ids'])  # because different tokenization\n",
    "\n",
    "# Transforming output\n",
    "pred_RNN = np.where(pred_RNN > threshold, 1, 0)\n",
    "pred_GRU = np.where(pred_GRU > threshold, 1, 0)\n",
    "pred_LSTM = np.where(pred_LSTM > threshold, 1, 0)\n",
    "pred_BERT = pred_BERT['logits'].argmax(axis=1)\n",
    "\n",
    "\n",
    "# Compare F1 scores\n",
    "print('RNN F1 score:')\n",
    "print(f1_score(actual_values,pred_RNN))\n",
    "\n",
    "print('GRU F1 score:')\n",
    "print(f1_score(actual_values,pred_GRU))\n",
    "\n",
    "print('LSTM F1 score:')\n",
    "print(f1_score(actual_values,pred_LSTM))\n",
    "\n",
    "\n",
    "print('BERT F1 score:')\n",
    "print(f1_score(actual_values,pred_BERT))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining Model on whole dataset\n",
    "\n",
    "As the distilbert model performed the best, we'll use this one to make our final submissions. In order to avoid 'wasting' any data, we'll now retrain it with the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize full dataset from scratch\n",
    "# Here tokenizer is already instanciated in this python file so we won't instanciate it again\n",
    "ds_full =  distil_bert_tokenizer(df.text_clean.tolist(),\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "# Tf dataset\n",
    "train_full = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(ds_full),\n",
    "    df.target.tolist()\n",
    "))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./models/distil_bert_weights were not used when initializing TFDistilBertForSequenceClassification: ['dropout_519']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./models/distil_bert_weights and are newly initialized: ['dropout_699']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "61/61 [==============================] - 2066s 33s/step - loss: 0.3760 - accuracy: 0.8465\n",
      "Epoch 2/2\n",
      "61/61 [==============================] - 2466s 40s/step - loss: 0.3322 - accuracy: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe21cd486d0>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 124\n",
    "# Reloading model to give it final name\n",
    "Final_model = TFDistilBertForSequenceClassification.from_pretrained(\"./models/distil_bert_weights\") \n",
    "\n",
    "# Compiling model \n",
    "# We use exactly the same as what was used previously\n",
    "# We could have decreased learning rate further given even more finetuning but given its already fairly low we won't touch it\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
    "\n",
    "Final_model.compile(optimizer=optimizer, \n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "# No callbacks this time as we'll train on same number of epochs as what was done previously given no validation dataset\n",
    "\n",
    "# Training\n",
    "Final_model.fit(train_full.shuffle(100).batch(BATCH_SIZE),\n",
    "          epochs=2, #The model had trained over 3 epochs, however it had restored the best weights (those at epoch 2) so we'll keep it this way \n",
    "          batch_size=BATCH_SIZE,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "Final_model.save_pretrained(\"./models/Final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>shakingit earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>d probably life arsenal yesterday eh eh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>nice hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No I don't like cold!</td>\n",
       "      <td>like cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOOOOOOOOO! Don't do that!</td>\n",
       "      <td>nooooooooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No don't tell me that!</td>\n",
       "      <td>tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What if?!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  ...                                    text_clean\n",
       "0    0  ...                     happen terrible car crash\n",
       "1    2  ...      hear earthquake different city stay safe\n",
       "2    3  ...  forest fire spot pond geese flee street save\n",
       "3    9  ...             apocalypse light spokane wildfire\n",
       "4   11  ...         typhoon soudelor kill 28 china taiwan\n",
       "5   12  ...                          shakingit earthquake\n",
       "6   21  ...       d probably life arsenal yesterday eh eh\n",
       "7   22  ...                                           hey\n",
       "8   27  ...                                      nice hat\n",
       "9   29  ...                                          fuck\n",
       "10  30  ...                                     like cold\n",
       "11  35  ...                                    nooooooooo\n",
       "12  42  ...                                          tell\n",
       "13  43  ...                                              \n",
       "14  45  ...                                       awesome\n",
       "\n",
       "[15 rows x 5 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "df_test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "# Same cleaning up as what was done on train dataset\n",
    "df_test['text_clean'] = df_test[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "df_test['text_clean'] = df_test[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "df_test['text_clean'] = df_test[\"text_clean\"].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)]))\n",
    "\n",
    "# Last check\n",
    "df_test.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing text\n",
    "ds_test =  distil_bert_tokenizer(df_test.text_clean.tolist(),\n",
    "                            truncation=True,\n",
    "                            padding=True,\n",
    "                            return_tensors=\"tf\")\n",
    "\n",
    "# Prediction\n",
    "preds = Final_model.predict(ds_test['input_ids'])['logits'].argmax(axis=1)\n",
    "\n",
    "# Saving submissions as .csv\n",
    "preds_df = pd.DataFrame()\n",
    "preds_df['target'] = preds\n",
    "preds_df.head(15) \n",
    "preds_df.to_csv('submission.csv', index=True, index_label='id') #formatting at required on Kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
