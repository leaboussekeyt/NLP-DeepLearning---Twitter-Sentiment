{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Embedding, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import seaborn as sns # check if still needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - EDA & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7552</td>\n",
       "      <td>5080</td>\n",
       "      <td>7613</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>221</td>\n",
       "      <td>3341</td>\n",
       "      <td>7503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>USA</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     keyword location  \\\n",
       "count    7613.000000        7552     5080   \n",
       "unique           NaN         221     3341   \n",
       "top              NaN  fatalities      USA   \n",
       "freq             NaN          45      104   \n",
       "mean     5441.934848         NaN      NaN   \n",
       "std      3137.116090         NaN      NaN   \n",
       "min         1.000000         NaN      NaN   \n",
       "25%      2734.000000         NaN      NaN   \n",
       "50%      5408.000000         NaN      NaN   \n",
       "75%      8146.000000         NaN      NaN   \n",
       "max     10873.000000         NaN      NaN   \n",
       "\n",
       "                                                     text      target  \n",
       "count                                                7613  7613.00000  \n",
       "unique                                               7503         NaN  \n",
       "top     11-Year-Old Boy Charged With Manslaughter of T...         NaN  \n",
       "freq                                                   10         NaN  \n",
       "mean                                                  NaN     0.42966  \n",
       "std                                                   NaN     0.49506  \n",
       "min                                                   NaN     0.00000  \n",
       "25%                                                   NaN     0.00000  \n",
       "50%                                                   NaN     0.00000  \n",
       "75%                                                   NaN     1.00000  \n",
       "max                                                   NaN     1.00000  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 'id' is not going to help us with the prediction, we can drop it straight away.\n",
    "Furthermore, we will look straight away into getting the prediction purely from the tweet content as we have a lot of missing value, so we'll drop the keyword & location column.\n",
    "\n",
    "It seems like we have some duplicate within the text column too, we'll look into that. Do they have the same target? If they do we'll remove them to have just one of each text, if they don't we need to look into why they don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "4290  #Allah describes piling up #wealth thinking it...       0\n",
       "4299  #Allah describes piling up #wealth thinking it...       0\n",
       "4312  #Allah describes piling up #wealth thinking it...       1\n",
       "6363  #Bestnaijamade: 16yr old PKK suicide bomber wh...       1\n",
       "6373  #Bestnaijamade: 16yr old PKK suicide bomber wh...       1\n",
       "...                                                 ...     ...\n",
       "6091  that horrible sinking feeling when youÛªve be...       1\n",
       "6103  that horrible sinking feeling when youÛªve be...       0\n",
       "6094  that horrible sinking feeling when youÛªve be...       0\n",
       "5641  wowo--=== 12000 Nigerian refugees repatriated ...       0\n",
       "5620  wowo--=== 12000 Nigerian refugees repatriated ...       1\n",
       "\n",
       "[179 rows x 2 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping aforementionned columns\n",
    "df.drop(['id','keyword','location'],axis=1,inplace=True)\n",
    "\n",
    "# Keep = False is to avoid that it keeps the first instance of each duplicate which would result in duplicates remaining\n",
    "#Looking at duplicated text\n",
    "df[df['text'].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some have the same target, some don't. If they have the same target, we'll clean up such that we have just one of each, if they don't we'll remove them altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[4290, 4299, 4312]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[4221, 4239, 4244]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[2830, 2831, 2832, 2833]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[4597, 4605, 4618]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[4232, 4235]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[3240, 3243, 3248, 3251, 3261, 3266]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hellfire is surrounded by desires so be carefu...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[4285, 4305, 4313]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hellfire! We donÛªt even want to think about ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[4306, 4320]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I Pledge Allegiance To The P.O.P.E. And The Bu...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[1214, 1365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In #islam saving a person is equal in reward t...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[6614, 6616]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mmmmmm I'm burning.... I'm burning buildings I...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[1197, 1331]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RT NotExplained: The only known image of infam...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[4379, 4381]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Prophet (peace be upon him) said 'Save you...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[4284, 4286, 4292, 4304, 4309, 4318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[610, 624, 630, 634]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Who is bringing the tornadoes and floods. Who ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[3985, 4013, 4019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>like for the music video I want some real acti...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[1221, 1349]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[6091, 6094, 6103, 6123]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[5620, 5641]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text    target  \\\n",
       "0   #Allah describes piling up #wealth thinking it...  0.333333   \n",
       "1   #foodscare #offers2go #NestleIndia slips into ...  0.666667   \n",
       "2   .POTUS #StrategicPatience is a strategy for #G...  0.750000   \n",
       "3   CLEARED:incident with injury:I-495  inner loop...  0.666667   \n",
       "4   Caution: breathing may be hazardous to your he...  0.500000   \n",
       "5   He came to a land which was engulfed in tribal...  0.333333   \n",
       "6   Hellfire is surrounded by desires so be carefu...  0.333333   \n",
       "7   Hellfire! We donÛªt even want to think about ...  0.500000   \n",
       "8   I Pledge Allegiance To The P.O.P.E. And The Bu...  0.500000   \n",
       "9   In #islam saving a person is equal in reward t...  0.500000   \n",
       "10  Mmmmmm I'm burning.... I'm burning buildings I...  0.500000   \n",
       "11  RT NotExplained: The only known image of infam...  0.500000   \n",
       "12  The Prophet (peace be upon him) said 'Save you...  0.333333   \n",
       "13                         To fight bioterrorism sir.  0.500000   \n",
       "14  Who is bringing the tornadoes and floods. Who ...  0.333333   \n",
       "15  like for the music video I want some real acti...  0.500000   \n",
       "16  that horrible sinking feeling when youÛªve be...  0.500000   \n",
       "17  wowo--=== 12000 Nigerian refugees repatriated ...  0.500000   \n",
       "\n",
       "                                   index  \n",
       "0                     [4290, 4299, 4312]  \n",
       "1                     [4221, 4239, 4244]  \n",
       "2               [2830, 2831, 2832, 2833]  \n",
       "3                     [4597, 4605, 4618]  \n",
       "4                           [4232, 4235]  \n",
       "5   [3240, 3243, 3248, 3251, 3261, 3266]  \n",
       "6                     [4285, 4305, 4313]  \n",
       "7                           [4306, 4320]  \n",
       "8                           [1214, 1365]  \n",
       "9                           [6614, 6616]  \n",
       "10                          [1197, 1331]  \n",
       "11                          [4379, 4381]  \n",
       "12  [4284, 4286, 4292, 4304, 4309, 4318]  \n",
       "13                  [610, 624, 630, 634]  \n",
       "14                    [3985, 4013, 4019]  \n",
       "15                          [1221, 1349]  \n",
       "16              [6091, 6094, 6103, 6123]  \n",
       "17                          [5620, 5641]  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['index'] = df.index # Will help to make list of originals index to pick up the data after\n",
    "\n",
    "aggs = {'target': 'mean',\n",
    "         'index':lambda x: list(x)} # Keep track of the index \n",
    "\n",
    "grouped_df = df[df['text'].duplicated(keep=False)].groupby('text').agg(aggs) \n",
    "\n",
    "# if target mean is not equal to 0 or 1 then the target is not consistent throughout the observations, let's see which one they are\n",
    "dif_targets = grouped_df[(grouped_df.target!=1) & (grouped_df.target!=0)].reset_index()\n",
    "dif_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take fix the target manually but given there is only 18 of them and some are ambiguous, we will proceed with removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We dropped 55 observations\n"
     ]
    }
   ],
   "source": [
    "# We'll save down the column of index as a list for us to drop them\n",
    "to_drop_dif_targets = dif_targets['index'].tolist()\n",
    "to_drop_dif_targets = [item for sublist in to_drop_dif_targets for item in sublist] #flatten nested list\n",
    "\n",
    "# Dropping duplicates with  target\n",
    "shape = df.shape[0]\n",
    "df.drop(to_drop_dif_targets,inplace=True)\n",
    "dropped =  shape - df.shape[0]\n",
    "\n",
    "print(f\"We dropped {dropped} observations\")\n",
    "\n",
    "\n",
    "# Cleaning up\n",
    "df.drop('index',axis=1,inplace=True) # no longer need that column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[6363, 6366, 6373, 6377, 6378, 6392]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#KCA #VoteJKT48ID 12News: UPDATE: A family of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2822, 2828]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Myanmar  Displaced #Rohingya at #Sittwe point...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2816, 2841]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Newswatch: 2 vehicles collided at Lock and La...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1704, 1725]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#SigAlert: North &amp;amp; Southbound 133 closed b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3790, 3795]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  #Bestnaijamade: 16yr old PKK suicide bomber wh...     1.0   \n",
       "1  #KCA #VoteJKT48ID 12News: UPDATE: A family of ...     1.0   \n",
       "2  #Myanmar  Displaced #Rohingya at #Sittwe point...     1.0   \n",
       "3  #Newswatch: 2 vehicles collided at Lock and La...     1.0   \n",
       "4  #SigAlert: North &amp; Southbound 133 closed b...     1.0   \n",
       "\n",
       "                                  index  \n",
       "0  [6363, 6366, 6373, 6377, 6378, 6392]  \n",
       "1                          [2822, 2828]  \n",
       "2                          [2816, 2841]  \n",
       "3                          [1704, 1725]  \n",
       "4                          [3790, 3795]  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now look at the one that have the same target\n",
    "# For those we'll keep only one of each to avoid any bias\n",
    "same_targets = grouped_df[(grouped_df.target==1) | (grouped_df.target==0)].reset_index()\n",
    "\n",
    "print(same_targets.shape)\n",
    "same_targets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We dropped 73 observations\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicates above keeping only one instance of each tweet\n",
    "\n",
    "shape = df.shape[0]\n",
    "\n",
    "df.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "dropped =  shape - df.shape[0]\n",
    "print(\"We dropped {} observations\".format(dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a final look at the dataset before we proceed to verify everything seems cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7485</td>\n",
       "      <td>7485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  target\n",
       "count   7485  7485.0\n",
       "unique  7485     NaN"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all').iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.574081\n",
       "1    0.425919\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True) # Checking for balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Forest fire near La Ronge Sask. Canada\n",
      "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "13,000 people receive #wildfires evacuation orders in California \n",
      "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
      "#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\n",
      "I'm on top of the hill and I can see a fire in the woods...\n",
      "There's an emergency evacuation happening now in the building across the street\n",
      "I'm afraid that the tornado is coming to our area...\n",
      "Three people died from the heat wave so far\n",
      "Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding\n",
      "#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count \n",
      "#Flood in Bago Myanmar #We arrived Bago\n",
      "Damage to school bus on 80 in multi car crash #BREAKING \n",
      "What's up man?\n",
      "I love fruits\n",
      "Summer is lovely\n",
      "My car is so fast\n",
      "What a goooooooaaaaaal!!!!!!\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True,drop=True) # deals with some missing index\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    print(df['text'][i]) # allows to see the complete text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks better, no more duplicated values. There is no missing values so we won't need to remove any more observations. \n",
    "\n",
    "The dataset is fairly balanced - about 42% of the tweets indicate a disaster.\n",
    "\n",
    "Let's clean up the texts a bit. There's a lot of links, but those will get out by themselves once we limit the vocabulary size as they are all unique.  \n",
    "\n",
    "Furthermore, we'll remove ponctuation, clean up if there's any double space, put everything in lowercase and remove the stop words, along with lemmatising the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>rockyfire update   california hwy 20 close dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain cause flash flooding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "      <td>m hill fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "      <td>s emergency evacuation happen building street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "      <td>m afraid tornado come area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "      <td>people die heat wave far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "      <td>haha south tampa getting flood hah wait second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>rain flood florida tampabay tampa 18 19 day ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "      <td>flood bago myanmar arrive bago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "      <td>damage school bus 80 multi car crash breaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "      <td>s man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "      <td>love fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>summer lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "      <td>car fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>goooooooaaaaaal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target  \\\n",
       "0   Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1              Forest fire near La Ronge Sask. Canada       1   \n",
       "2   All residents asked to 'shelter in place' are ...       1   \n",
       "3   13,000 people receive #wildfires evacuation or...       1   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "5   #RockyFire Update => California Hwy. 20 closed...       1   \n",
       "6   #flood #disaster Heavy rain causes flash flood...       1   \n",
       "7   I'm on top of the hill and I can see a fire in...       1   \n",
       "8   There's an emergency evacuation happening now ...       1   \n",
       "9   I'm afraid that the tornado is coming to our a...       1   \n",
       "10        Three people died from the heat wave so far       1   \n",
       "11  Haha South Tampa is getting flooded hah- WAIT ...       1   \n",
       "12  #raining #flooding #Florida #TampaBay #Tampa 1...       1   \n",
       "13            #Flood in Bago Myanmar #We arrived Bago       1   \n",
       "14  Damage to school bus on 80 in multi car crash ...       1   \n",
       "15                                     What's up man?       0   \n",
       "16                                      I love fruits       0   \n",
       "17                                   Summer is lovely       0   \n",
       "18                                  My car is so fast       0   \n",
       "19                       What a goooooooaaaaaal!!!!!!       0   \n",
       "\n",
       "                                           text_clean  \n",
       "0                deed reason earthquake allah forgive  \n",
       "1               forest fire near la ronge sask canada  \n",
       "2   resident ask shelter place notify officer evac...  \n",
       "3   13000 people receive wildfire evacuation order...  \n",
       "4   got send photo ruby alaska smoke wildfires pou...  \n",
       "5   rockyfire update   california hwy 20 close dir...  \n",
       "6   flood disaster heavy rain cause flash flooding...  \n",
       "7                                    m hill fire wood  \n",
       "8       s emergency evacuation happen building street  \n",
       "9                          m afraid tornado come area  \n",
       "10                           people die heat wave far  \n",
       "11  haha south tampa getting flood hah wait second...  \n",
       "12  rain flood florida tampabay tampa 18 19 day ve...  \n",
       "13                     flood bago myanmar arrive bago  \n",
       "14      damage school bus 80 multi car crash breaking  \n",
       "15                                              s man  \n",
       "16                                         love fruit  \n",
       "17                                      summer lovely  \n",
       "18                                           car fast  \n",
       "19                                    goooooooaaaaaal  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Loading english language elements from spacy\n",
    "\n",
    "df['text_clean'] = df[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "df['text_clean'] = df[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "df['text_clean'] = df[\"text_clean\"].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)]))\n",
    "\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data & tokenizing the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the dataset is already split into train / test, the test one is only to submit to Kaggle and it does not have a target, making it complicated to evalue our model. \n",
    "\n",
    "We should thus train/test the train dataset to choose the best model. We'll also keep 10% for final testing to have an idea of how we expect our model to perform. \n",
    "The proportion are thus as follow:\n",
    "\n",
    "    * Train: 75%\n",
    "\n",
    "    * Validation: 15%\n",
    "    \n",
    "    * Test: 10%\n",
    "\n",
    "We'll also both set a random state, to ensure we are always working with the same split, and stratify with respect to the target as we are working with a classification problem. \n",
    "\n",
    "However, before submitting to Kaggle, we'll retrain the model on the entire dataset as to not 'waste' any data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "(5613,) (5613,)\n",
      "Validation:\n",
      "(1124,) (1124,)\n",
      "Testing:\n",
      "(748,) (748,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df.text_clean, df.target ,test_size=0.25,random_state = 0, stratify = df.target)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test ,test_size=0.6,random_state = 0, stratify = Y_test) # 0.6 * 0.25 = 0.15\n",
    " \n",
    "print('Training:')\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print('Validation:')\n",
    "print(X_valid.shape, Y_valid.shape)\n",
    "print('Testing:')\n",
    "print(X_test.shape,Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000, oov_token=\"<OOV>\") # instanciate the tokenizer\n",
    "\n",
    "tokenizer.fit_on_texts(X_train) # fitting tokenizer only on train dataset to prevent any data leakage\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "\n",
    "\n",
    "# Dropping empty observations which occur when no word in the tweet are in the dictionnary --\n",
    "#  think no longer needed now that oov_token\n",
    "#X_valid = [sublist for sublist in X_valid if len(sublist)!=0]\n",
    "#X_train = [sublist for sublist in X_train if len(sublist)!=0]\n",
    "#X_test = [sublist for sublist in X_test if len(sublist)!=0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is  25\n"
     ]
    }
   ],
   "source": [
    "# Finding out the max length out of all the texts\n",
    "MAX_LENGTH = len(max(X_train + X_test + X_valid, key =  lambda i: len(i) ))\n",
    "print(\"The max length is \",MAX_LENGTH)\n",
    "\n",
    "# Padding the sequences \n",
    "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, padding=\"post\", maxlen= MAX_LENGTH)\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding=\"post\", maxlen= MAX_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\", maxlen= MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[777   1  14 ...   0   0   0]\n",
      " [247  17 134 ...   0   0   0]\n",
      " [778  92   5 ...   0   0   0]\n",
      " ...\n",
      " [  1   1 910 ...   0   0   0]\n",
      " [ 51 310   1 ...   0   0   0]\n",
      " [104   1 566 ...   0   0   0]], shape=(124, 25), dtype=int32) tf.Tensor(\n",
      "[1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
      " 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0], shape=(124,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 124\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train,Y_train)).batch(BATCH_SIZE)\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test,Y_test.values)).batch(BATCH_SIZE)\n",
    "valid = tf.data.Dataset.from_tensor_slices((X_test,Y_test.values)).batch(BATCH_SIZE)\n",
    "\n",
    "for text, target in train.take(1):\n",
    "  print(text, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal all the way below --- TO ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000) # instanciate the tokenizer\n",
    "tokenizer.fit_on_texts(df.text_clean)\n",
    "df[\"text_encoded\"] = tokenizer.texts_to_sequences(df.text_clean)\n",
    "\n",
    "# Let's remove observations which are empty once encoded with the chosen number of words\n",
    "df[\"len_text\"] = df[\"text_encoded\"].apply(lambda x: len(x))\n",
    "df = df[df[\"len_text\"]!=0]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding sequences such that each has the same length\n",
    "text_pad = tf.keras.preprocessing.sequence.pad_sequences(df.text_encoded, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the dataset is already split into train / test, the test one is only to submit to Kaggle and it does not have a target, making it complicated to evalue our model. \n",
    "\n",
    "We should thus train/test the train dataset to choose the best model. We'll also keep 10% for final testing to have an idea of how we expect our model to perform. \n",
    "The proportion are thus as follow:\n",
    "\n",
    "    * Train: 75%\n",
    "\n",
    "    * Validation: 15%\n",
    "    \n",
    "    * Test: 10%\n",
    "\n",
    "We'll also both set a random state, to ensure we are always working with the same split, and stratify with respect to the target as we are working with a classification problem. \n",
    "\n",
    "However, before submitting to Kaggle, we'll retrain the model on the entire dataset as to not 'waste' any data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(text_pad, df.target ,test_size=0.25,random_state = 0, stratify = df.target)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test ,test_size=0.6,random_state = 0, stratify = Y_test) # 0.6 * 0.25 = 0.15\n",
    " \n",
    "print('Training:')\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print('Validation:')\n",
    "print(X_valid.shape, Y_valid.shape)\n",
    "print('Testing:')\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 124\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train,Y_train)).batch(BATCH_SIZE)\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test,Y_test.values)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "for text, target in train.take(1):\n",
    "  print(text, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is all pre-processed, we can move on to the model.\n",
    "\n",
    "# Recurrent Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                  # Word Embedding layer           \n",
    "                  Embedding(vocab_size, 64,input_shape=[MAX_LENGTH,]),\n",
    "                  # Gobal average pooling\n",
    "                  SimpleRNN(units=64, return_sequences=True), # maintains the sequential nature\n",
    "                  SimpleRNN(units=32, return_sequences=False), # returns the last output\n",
    "                  # Dense layers once the data is flat\n",
    "                  Dense(16, activation='relu'),\n",
    "                  Dense(8, activation='relu'),\n",
    "\n",
    "                  # output layer with as many neurons as the number of classes\n",
    "                  # for the target variable and softmax activation\n",
    "                  Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 25, 64)            1051008   \n",
      "                                                                 \n",
      " simple_rnn_20 (SimpleRNN)   (None, 25, 64)            8256      \n",
      "                                                                 \n",
      " simple_rnn_21 (SimpleRNN)   (None, 32)                3104      \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,063,041\n",
      "Trainable params: 1,063,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss = tf.keras.losses.BinaryCrossentropy(), # bc binary case here\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "46/46 [==============================] - 5s 54ms/step - loss: 0.6728 - binary_accuracy: 0.5591 - val_loss: 0.6075 - val_binary_accuracy: 0.6765\n",
      "Epoch 2/25\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 0.5346 - binary_accuracy: 0.7454 - val_loss: 0.5271 - val_binary_accuracy: 0.7447\n",
      "Epoch 3/25\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 0.4142 - binary_accuracy: 0.8222 - val_loss: 0.4947 - val_binary_accuracy: 0.7660\n",
      "Epoch 4/25\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 0.3482 - binary_accuracy: 0.8628 - val_loss: 0.5489 - val_binary_accuracy: 0.7540\n",
      "Epoch 5/25\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.3130 - binary_accuracy: 0.8813 - val_loss: 0.5927 - val_binary_accuracy: 0.7487\n",
      "Epoch 6/25\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 0.2765 - binary_accuracy: 0.8990 - val_loss: 0.6840 - val_binary_accuracy: 0.7152\n",
      "Epoch 7/25\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2622 - binary_accuracy: 0.9025 - val_loss: 0.6720 - val_binary_accuracy: 0.7366\n",
      "Epoch 8/25\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 0.2463 - binary_accuracy: 0.9099 - val_loss: 0.6464 - val_binary_accuracy: 0.7620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea0d6e3df0>"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,\n",
    "          epochs=25, \n",
    "          validation_data=valid,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/model_RNN.h5\") # model_simpleRNN_1 was done on batch size = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = tf.keras.Sequential([\n",
    "                  Embedding(vocab_size+1, 64, input_shape=[MAX_LENGTH,],name=\"embedding\"),\n",
    "                  GRU(units=64, return_sequences=True), # maintains the sequential nature\n",
    "                  GRU(units=32, return_sequences=False), # returns the last output\n",
    "                  Dense(16, activation='relu'),\n",
    "                  Dense(8, activation='relu'),\n",
    "\n",
    "                  Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 64)            1051072   \n",
      "                                                                 \n",
      " gru_10 (GRU)                (None, 25, 64)            24960     \n",
      "                                                                 \n",
      " gru_11 (GRU)                (None, 32)                9408      \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,086,113\n",
      "Trainable params: 1,086,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "model_gru.compile(optimizer=optimizer, \n",
    "              loss = tf.keras.losses.BinaryCrossentropy(), # bc binary case here\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "46/46 [==============================] - 13s 107ms/step - loss: 0.6831 - binary_accuracy: 0.5740 - val_loss: 0.6837 - val_binary_accuracy: 0.5749\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 4s 91ms/step - loss: 0.6761 - binary_accuracy: 0.5753 - val_loss: 0.6436 - val_binary_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 4s 88ms/step - loss: 0.5103 - binary_accuracy: 0.7632 - val_loss: 0.4589 - val_binary_accuracy: 0.7995\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 4s 92ms/step - loss: 0.4286 - binary_accuracy: 0.8183 - val_loss: 0.4650 - val_binary_accuracy: 0.7968\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 4s 94ms/step - loss: 0.3898 - binary_accuracy: 0.8388 - val_loss: 0.4740 - val_binary_accuracy: 0.7981\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 4s 90ms/step - loss: 0.3785 - binary_accuracy: 0.8434 - val_loss: 0.5046 - val_binary_accuracy: 0.7981\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 4s 89ms/step - loss: 0.3701 - binary_accuracy: 0.8516 - val_loss: 0.5312 - val_binary_accuracy: 0.7914\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 5s 115ms/step - loss: 0.3631 - binary_accuracy: 0.8536 - val_loss: 0.5087 - val_binary_accuracy: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea0c5062e0>"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.fit(train,\n",
    "              epochs=20, \n",
    "              validation_data=valid,\n",
    "              callbacks=callbacks\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.save('models/model_GRU.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = tf.keras.Sequential([\n",
    "                  Embedding(vocab_size+1, 64, input_shape=[MAX_LENGTH],name=\"embedding\"),\n",
    "                  LSTM(units=64, return_sequences=True), # maintains the sequential nature\n",
    "                  LSTM(units=32, return_sequences=False), # returns the last output\n",
    "                  Dense(16, activation='relu'),\n",
    "                  Dense(8, activation='relu'),\n",
    "                  Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 64)            1051072   \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, 25, 64)            33024     \n",
      "                                                                 \n",
      " lstm_21 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,097,185\n",
      "Trainable params: 1,097,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "model_lstm.compile(optimizer=optimizer, \n",
    "              loss = tf.keras.losses.BinaryCrossentropy(), # bc binary case here\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 10, \n",
    "                                        restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "46/46 [==============================] - 10s 125ms/step - loss: 0.6570 - binary_accuracy: 0.5756 - val_loss: 0.5929 - val_binary_accuracy: 0.7393\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 7s 143ms/step - loss: 0.5511 - binary_accuracy: 0.7816 - val_loss: 0.5733 - val_binary_accuracy: 0.7393\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 4s 97ms/step - loss: 0.4948 - binary_accuracy: 0.8014 - val_loss: 0.5297 - val_binary_accuracy: 0.7594\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 5s 99ms/step - loss: 0.4449 - binary_accuracy: 0.8163 - val_loss: 0.4886 - val_binary_accuracy: 0.7981\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.4209 - binary_accuracy: 0.8247 - val_loss: 0.5131 - val_binary_accuracy: 0.7901\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.3942 - binary_accuracy: 0.8422 - val_loss: 0.5077 - val_binary_accuracy: 0.7807\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 5s 100ms/step - loss: 0.3752 - binary_accuracy: 0.8514 - val_loss: 0.5041 - val_binary_accuracy: 0.7767\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 5s 107ms/step - loss: 0.3615 - binary_accuracy: 0.8580 - val_loss: 0.5296 - val_binary_accuracy: 0.7607\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 5s 98ms/step - loss: 0.3583 - binary_accuracy: 0.8584 - val_loss: 0.5425 - val_binary_accuracy: 0.7594\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 5s 107ms/step - loss: 0.3558 - binary_accuracy: 0.8596 - val_loss: 0.5401 - val_binary_accuracy: 0.7714\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3678 - binary_accuracy: 0.8502 - val_loss: 0.5185 - val_binary_accuracy: 0.7781\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.3549 - binary_accuracy: 0.8568 - val_loss: 0.5392 - val_binary_accuracy: 0.7607\n",
      "Epoch 13/30\n",
      "46/46 [==============================] - 5s 111ms/step - loss: 0.3296 - binary_accuracy: 0.8708 - val_loss: 0.5637 - val_binary_accuracy: 0.7674\n",
      "Epoch 14/30\n",
      "46/46 [==============================] - 5s 118ms/step - loss: 0.3265 - binary_accuracy: 0.8708 - val_loss: 0.5822 - val_binary_accuracy: 0.7620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea07b0cc70>"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(train,\n",
    "                epochs = 30,\n",
    "                validation_data = valid,\n",
    "                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save('models/model_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model\n",
    "\n",
    "Source used: https://swatimeena989.medium.com/bert-text-classification-using-keras-903671e0207d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "# from transformers import TFBertModel, BertConfig # Think those two are not needed but to double check if all works this way\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text:\n",
      "{'input_ids': [101, 3224, 2543, 2379, 2474, 6902, 3351, 21871, 2243, 2710, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decoded text:\n",
      "[CLS] forest fire near la ronge sask canada [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "test_encode = df['text_clean'][1] # copy text from first few tweets and check how it tokenizes\n",
    "\n",
    "tokenized_sequence = bert_tokenizer.encode_plus(test_encode,\n",
    "                                                add_special_tokens = True,\n",
    "                                                max_length =18, # Size of the longest tweet\n",
    "                                                pad_to_max_length = True,\n",
    "                                                truncation = True,\n",
    "                                                return_attention_mask = True,\n",
    "                                                )\n",
    "\n",
    "\n",
    "print('Tokenized text:')\n",
    "print(tokenized_sequence)\n",
    "\n",
    "print('Decoded text:')\n",
    "\n",
    "print(bert_tokenizer.decode(tokenized_sequence['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julietteboussekeyt/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BERT has its own tokenizer hence we need to redo it \n",
    "# Tokenize all the dataset\n",
    "\n",
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for tweet in df['text_clean']:\n",
    "    bert_inp=bert_tokenizer.encode_plus(tweet,\n",
    "                                        add_special_tokens = True,\n",
    "                                        max_length =MAX_LENGTH,\n",
    "                                        pad_to_max_length = True,\n",
    "                                        return_attention_mask = True\n",
    "                                        )\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "    \n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)\n",
    "labels=np.array(df['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, Mask_train, Mask_test = train_test_split(\n",
    "                                                            input_ids,\n",
    "                                                            labels,\n",
    "                                                            attention_masks,\n",
    "                                                            test_size=0.25,\n",
    "                                                            random_state = 0, \n",
    "                                                            stratify = labels\n",
    "                                                            )\n",
    "\n",
    "X_test, X_valid, Y_test, Y_valid, Mask_test, Mask_valid = train_test_split(\n",
    "                                                            X_test,\n",
    "                                                            Y_test,\n",
    "                                                            Mask_test,\n",
    "                                                            test_size=0.6,\n",
    "                                                            random_state = 0,\n",
    "                                                            stratify = Y_test # 0.6 * 0.25 = 0.15\n",
    "                                                            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "log_dir='tensorboard_data/tb_bert'\n",
    "model_save_path='./bert_model.h5'\n",
    "\n",
    "### see if can change the below to other callbacks\n",
    "#callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,\n",
    "#save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "\n",
    "callbacks = EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "for layer in bert_model.layers[:-1]:\n",
    "    layer.trainable = False  # Only change the last dense layer, we are just finetuning this model not training it from scratch\n",
    "\n",
    "print('\\nBert Model',bert_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "bert_model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/44 [=========>....................] - ETA: 3:21 - loss: 0.5388 - accuracy: 0.7260"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-8b96fc7a4d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMask_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMask_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_model.fit([X_train,Mask_train],Y_train,batch_size=128,epochs=3,validation_data=([X_valid,Mask_valid],Y_valid),callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN F1 score:\n",
      "0.6741154562383612\n",
      "GRU F1 score:\n",
      "0.7440273037542663\n",
      "LSTM F1 score:\n",
      "0.7504132231404959\n"
     ]
    }
   ],
   "source": [
    "# Load the different models\n",
    "RNN_model = tf.keras.models.load_model('models/model_RNN.h5')\n",
    "GRU_model = tf.keras.models.load_model('models/model_GRU.h5')\n",
    "LSTM_model = tf.keras.models.load_model('models/model_lstm.h5')\n",
    "bert_model = tf.keras.models.load_model('bert_model.h5')  ## change path after\n",
    "\n",
    "## Setting up \n",
    "actual_values = Y_test \n",
    "target_names = ['No disaster','Disaster']\n",
    "threshold = 0.5\n",
    "\n",
    "# Get the predictions\n",
    "pred_RNN  = RNN_model.predict(X_test)\n",
    "pred_GRU  = GRU_model.predict(X_test)\n",
    "pred_LSTM = LSTM_model.predict(X_test)\n",
    "\n",
    "pred_RNN = np.where(pred_RNN > threshold, 1, 0)\n",
    "pred_GRU = np.where(pred_GRU > threshold, 1, 0)\n",
    "pred_LSTM = np.where(pred_LSTM > threshold, 1, 0)\n",
    "\n",
    "# Compare F1 scores\n",
    "print('RNN F1 score:')\n",
    "print(f1_score(actual_values,pred_RNN))\n",
    "\n",
    "print('GRU F1 score:')\n",
    "print(f1_score(actual_values,pred_GRU))\n",
    "\n",
    "print('LSTM F1 score:')\n",
    "print(f1_score(actual_values,pred_LSTM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN F1 score:\n",
    "0.6958105646630236\n",
    "GRU F1 score:\n",
    "0.7416107382550335\n",
    "LSTM F1 score:\n",
    "0.7204116638078902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.embeddings.Embedding at 0x7fea1d3dad30>,\n",
       " <keras.layers.recurrent.SimpleRNN at 0x7fea3549da60>,\n",
       " <keras.layers.recurrent.SimpleRNN at 0x7fea12f8fd00>,\n",
       " <keras.layers.core.dense.Dense at 0x7fea247cba60>,\n",
       " <keras.layers.core.dense.Dense at 0x7fea1d6150a0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fea1d49da90>]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = tf.keras.models.load_model(\"model.GRU_1.h5\")\n",
    "bert_model.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.7196029776674938\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Disaster       0.78      0.84      0.81       849\n",
      " No disaster       0.76      0.69      0.72       635\n",
      "\n",
      "    accuracy                           0.77      1484\n",
      "   macro avg       0.77      0.76      0.76      1484\n",
      "weighted avg       0.77      0.77      0.77      1484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_save_path='./bert_model.h5'\n",
    "\n",
    "trained_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
    "trained_model.load_weights(model_save_path)\n",
    "\n",
    "preds = trained_model.predict([val_inp,val_mask],batch_size=32)\n",
    "#pred_labels = preds.argmax(axis=1)\n",
    "\n",
    "target_names = ['Disaster','No disaster']\n",
    "pred_labels = preds['logits'].argmax(axis=1)\n",
    "f1 = f1_score(val_label,pred_labels)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "print(classification_report(val_label,pred_labels,target_names=target_names))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1484, 2)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.load_model(\"model.GRU_1.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
